## Installing a OpenShift Cluster in a Connected Network on Baremetal Machines


# Extract All binaries and place in bin directory. 

[root@bastion ~]# tar xf ccoctl-linux-4.17.2.tar.gz -C /usr/bin
[root@bastion ~]# tar xf oc-mirror.tar.gz -C /usr/bin
[root@bastion ~]# tar xf openshift-client-linux-4.17.2.tar.gz  -C /usr/bin
[root@bastion ~]# tar xf openshift-install-linux-4.17.2.tar.gz -C /usr/bin

[root@bastion ~]# mv butane /usr/bin
[root@bastion ~]# mv helm-linux-amd64  /usr/bin/helm
[root@bastion ~]# chmod 777 /usr/bin/oc-mirror /usr/bin/helm /usr/bin/butane 

### Installing a OpenShift Cluster Setup.

Generating an SSH private key and adding it to the agent

    [root@bastion ~]# ssh-keygen -t ed25519 -N ''
    Generating public/private ed25519 key pair.
    Enter file in which to save the key (/root/.ssh/id_ed25519):
    Created directory '/root/.ssh'.
    Your identification has been saved in /root/.ssh/id_ed25519.
    Your public key has been saved in /root/.ssh/id_ed25519.pub.
    The key fingerprint is:
    SHA256:z3mXx9Pe/hPC6dCkVtQB3NOc05NqoxTVnysV+OSAD6I root@bastion.lab.example.com
    The key's randomart image is:
    +--[ED25519 256]--+
    |            +o*o*|
    |         . + = %+|
    |        . . = * B|
    |       E   . B =.|
    |        S . O + .|
    |         o * * =.|
    |          = + =.=|
    |           . o +o|
    |               .B|
    +----[SHA256]-----+
    
    [root@bastion ~]# cat .ssh/id_ed25519.pub 
    ssh-ed25519 REDACTED root@bastion.lab.example.com
    
### Manually creating the installation configuration file

cloud.openshift.com/openshift/downloads download for pull secret. 

    [root@bastion ocp4]# cat install-config.yaml 
    apiVersion: v1
    baseDomain: example.com
    compute:
    - hyperthreading: Enabled
      name: worker 
      replicas: 0 
    controlPlane:
      hyperthreading: Enabled
      name: master
      replicas: 1
    metadata:
      name: lab
    networking:
      clusterNetwork:
        - cidr: 10.128.0.0/14
          hostPrefix: 23
      networkType: OVNKubernetes
      serviceNetwork:
        - 172.30.0.0/16
    platform:
    	none: {}
    fips: false
    pullSecret: '{"auths":{"cloud.openshift.com":{"auth":"b3BlbnNoaW<redacted>FhaFRMOWF3MXVTVXk1c3l3aExFR1VTN0xLc1VaTQ==","email":"sanuragi@redhat.com"}}}'
    sshKey: "ssh-ed25519 AAAAC3NzaC1{<REDACTED>}DIUeZJkdjTaMRkzn root@bastion.lab.example.com"


### Creating the Openshift/Kubernetes manifest

    [root@bastion ~]# ./openshift-install create manifests --dir ocp4/
    INFO Consuming Install Config from target directory 
    WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings 
    INFO Manifests created in: lab/manifests and lab/openshift 

### Creating the Openshift/Kubernetes cluster scheduling false. 

    [root@bastion ocp4]# ls
    manifests  openshift

    [root@bastion ocp4]# cd manifests/

    [root@bastion manifests]# ls
    cluster-config.yaml                   cluster-network-01-crd.yml       cvo-overrides.yaml                  kube-system-configmap-root-ca.yaml        
    cluster-dns-02-config.yml             cluster-network-02-config.yml    Images/image-content-source-policy-0.yaml  machine-config-server-tls-secret.yaml     
    cluster-infrastructure-02-config.yml  cluster-proxy-01-config.yaml     Images/image-content-source-policy-1.yaml  openshift-config-secret-pull-secret.yaml  
    cluster-ingress-02-config.yml         cluster-scheduler-02-config.yml  kube-cloud-config.yaml              user-ca-bundle-config.yaml

    [root@bastion manifests]# vim cluster-scheduler-02-config.yml
    apiVersion: config.openshift.io/v1
    kind: Scheduler
    metadata:
      creationTimestamp: null
      name: cluster
    spec:
      mastersSchedulable: true
      policy:
        name: ""
    status: {}

### Creating the Openshift/Kubernetes ignition files.

    [root@bastion ~]# ./openshift-install create ignition-configs --dir=ocp4
    INFO Consuming Openshift Manifests from target directory 
    INFO Consuming OpenShift Install (Manifests) from target directory 
    INFO Consuming Worker Machines from target directory 
    INFO Consuming Common Manifests from target directory 
    INFO Consuming Master Machines from target directory 
    INFO Ignition-Configs created in: lab and lab/auth 

### Install and Configure Apache HTTPD Server

    [root@bastion ~]# yum install httpd -y

### Create new directory lab. 

    [root@bastion ~]# mkdir /var/www/html/ocp4

### Copying all ignition files to web directory. 

    [root@bastion ~]# cp -rf ocp4/*.ign /var/www/html/ocp4/

### Changing Ownership and Permissions.

    [root@bastion ~]# chown apache:apache -R /var/www/html/ocp4/
    [root@bastion ~]# chmod 644 -R /var/www/html/ocp4/*.ign
    
    [root@bastion html]# cd /var/www/html/ocp4/
    [root@bastion ocp4]# ll
    total 284
    -rw-r--r-- 1 apache apache 280395 Jan 24 23:37 bootstrap.ign
    -rw-r--r-- 1 apache apache   1728 Jan 24 23:37 master.ign
    -rw-r--r-- 1 apache apache   1728 Jan 24 23:37 worker.ign
 
### Checking output of web server. 
       
    [root@bastion lab]# curl http://bastion.lab.example.com/ocp4/
    <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
    <html>
     <head>
      <title>Index of /lab</title>
     </head>
     <body>
    <h1>Index of /lab</h1>
      <table>
       <tr><th valign="top"><img src="/icons/blank.gif" alt="[ICO]"></th><th><a href="?C=N;O=D">Name</a></th><th><a href="?C=M;O=A">Last modified</a></th><th><a href="?C=S;O=A">Size</a></th><th><a href="?C=D;O=A">Description</a></th></tr>
       <tr><th colspan="5"><hr></th></tr>
    <tr><td valign="top"><img src="/icons/back.gif" alt="[PARENTDIR]"></td><td><a href="/">Parent Directory</a>       </td><td>&nbsp;</td><td align="right">  - </td><td>&nbsp;</td></tr>
    <tr><td valign="top"><img src="/icons/unknown.gif" alt="[   ]"></td><td><a href="bootstrap.ign">bootstrap.ign</a>          </td><td align="right">2024-01-24 23:37  </td><td align="right">274K</td><td>&nbsp;</td></tr>
    <tr><td valign="top"><img src="/icons/unknown.gif" alt="[   ]"></td><td><a href="master.ign">master.ign</a>             </td><td align="right">2024-01-24 23:37  </td><td align="right">1.7K</td><td>&nbsp;</td></tr>
    <tr><td valign="top"><img src="/icons/unknown.gif" alt="[   ]"></td><td><a href="worker.ign">worker.ign</a>             </td><td align="right">2024-01-24 23:37  </td><td align="right">1.7K</td><td>&nbsp;</td></tr>
       <tr><th colspan="5"><hr></th></tr>
    </table>
    </body></html>

## Preparing Bootstrap Node 

### Boot Bootstrap node using rhcos image.

    boot -> rhcos live 
                |_ sudo nmtui 
                        |_ edit a connection 
                            |_ ens34 -> edit 
                                |_ profile name: ens34 
                                |_IPv4 CONFIGURATION <Manual>
                                |_Addresses 192.168.1.6/24
                                |_Gateway 192.168.1.1
                                │_DNS servers 192.168.1.3
                                │_            8.8.8.8
                                |_ ok 
                        |_ activate a connection 
                                |_ ens34 : deactivate -> activate
    
    $ sudo coreos-installer install /dev/sda --insecure-ignition --copy-network --ignition-url http://192.168.1.4/ocp4/bootstrap.ign 
    $ reboot  
    
### Boot Master node using rhcos image.
    boot -> rhcos live 
                |_ sudo nmtui 
                        |_ edit a connection 
                            |_ ens34 -> edit 
                                |_ profile name: ens34 
                                |_IPv4 CONFIGURATION <Manual>
                                |_Addresses 192.168.1.7/24
                                |_Gateway 192.168.1.1
                                │_DNS servers 192.168.1.3
                                │_            8.8.8.8
                                |_ ok 
                        |_ activate a connection 
                                |_ ens34 : deactivate -> activate
    
    $ sudo coreos-installer install /dev/sda --insecure-ignition --copy-network --ignition-url http://192.168.1.4/ocp4/master.ign 
    $ reboot  
    

# Openshift 4.17 Bootstrapping

Login to bootstrap node. 

    [root@bastion ~]# ssh core@192.168.1.6
    Warning: Permanently added '192.168.1.6' (ECDSA) to the list of known hosts.
    Red Hat Enterprise Linux CoreOS 412.86.202308081039-0
      Part of OpenShift 4.12, RHCOS is a Kubernetes native operating system
      managed by the Machine Config Operator (`clusteroperator/machine-config`).
    
    WARNING: Direct SSH access to machines is not recommended; instead,
    make configuration changes via `machineconfig` objects:
      https://docs.openshift.com/container-platform/4.12/architecture/architecture-rhcos.html
    
    ---
    This is the bootstrap node; it will be destroyed when the master is fully up.
    
    The primary services are release-image.service followed by bootkube.service. To watch their status, run e.g.
    
      journalctl -b -f -u release-image.service -u bootkube.service

    [core@boot ~]$  journalctl -b -f -u release-image.service -u bootkube.service
    -- Logs begin at Wed 2024-01-24 19:34:14 UTC. --
    Jan 24 19:34:29 boot.lab.example.com systemd[1]: Starting Download the OpenShift Release Image...
    Jan 24 19:34:29 boot.lab.example.com release-Images/image-download.sh[1904]: Pulling bastion.lab.example.com:8443/lab/openshift4@sha256:fcc9920ba10ebb02c69bdd9cd597273260eeec1b22e9ef9986a47f4874a21253...
    Jan 24 19:34:29 boot.lab.example.com release-Images/image-download.sh[1977]: 4be070dd54889436046224a7993fea8e507fd4658785912352688456b6359386
    Jan 24 19:34:29 boot.lab.example.com systemd[1]: Started Download the OpenShift Release Image.
    Jan 24 19:34:31 boot.lab.example.com systemd[1]: Started Bootstrap a Kubernetes cluster.

## Monitor the Bootstrap Process

You can monitor the bootstrap process from the bastion host at different log levels (debug, error, info)

    [root@bastion ~]# ~/openshift-install --dir ~/lab wait-for bootstrap-complete --log-level=debug

Once bootstrapping is complete the boot.lab.example.com node can be removed

## Remove the Bootstrap Node

Remove all references to the ocp-bootstrap host from the /etc/haproxy/haproxy.cfg file

    # Two entries
    [root@bastion ~]# vim /etc/haproxy/haproxy.cfg

 Restart HAProxy - If you are still watching HAProxy stats console you will see that the boostrap host has been removed from the backends.

    [root@bastion ~]# systemctl reload haproxy

The ocp-bootstrap host can now be safely shutdown and deleted from the VMware ESXi Console, the host is no longer required



# Openshift Skeleton Deployment

## Wait for installation to complete

    [root@bastion ~]# ~/openshift-install --dir ~/ocp4 wait-for install-complete

Continue to join the worker nodes to the cluster in a new tab whilst waiting for the above command to complete


Setup 'oc' and 'kubectl' clients on the ocp-svc machine

    [root@bastion ~]# export KUBECONFIG=~/ocp4/auth/kubeconfig

Test auth by viewing cluster nodes

    [root@bastion ~]# oc get node
    NAME                      STATUS   ROLES                         AGE    VERSION
    master1.lab.example.com   Ready    control-plane,master,worker   3h2m   v1.29.9+5865c5b
    master2.lab.example.com   Ready    control-plane,master,worker   3h3m   v1.29.9+5865c5b
    master3.lab.example.com   Ready    control-plane,master,worker   3h3m   v1.29.9+5865c5b

## View and approve pending CSRs

View CSRs

    [root@bastion ~]# oc get csr

Approve all pending CSRs

    [root@bastion ~]# oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve

Wait for kubelet-serving CSRs and approve them too with the same command

    [root@bastion ~]# oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve

### Checking cluster operator status.

    [root@baston ~]# oc get co 
    NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
    authentication                             4.17.4   True        False         False      36s     
    baremetal                                  4.17.4   True        False         False      8d      
    cloud-controller-manager                   4.17.4    True        False         False      8d      
    cloud-credential                           4.17.4   True        False         False      8d      
    cluster-autoscaler                         4.17.4   True        False         False      8d      
    config-operator                            4.17.4    True        False         False      8d      
    console                                    4.17.4   True        False         False      45m     
    control-plane-machine-set                  4.17.4    True        False         False      8d      
    csi-snapshot-controller                    4.17.4   True        False         False      8d      
    dns                                        4.17.4   True        False         False      25m     
    etcd                                       4.17.4    True        False         False      8d      
    image-registry                             4.17.4   True        False         False      8d      
    ingress                                    4.17.4   True        False         False      8d      
    insights                                   4.17.4   True        False         False      8d      
    kube-apiserver                             4.17.4    True        False         False      8d      
    kube-controller-manager                    4.17.4    True        False         False      8d      
    kube-scheduler                             4.17.4    True        False         False      8d      
    kube-storage-version-migrator              4.17.4   True        False         False      8d      
    machine-api                                4.17.4   True        False         False      8d      
    machine-approver                           4.17.4   True        False         False      8d      
    machine-config                             4.17.4   True        False         False      8d      
    marketplace                                4.17.4   True        False         False      8d      
    monitoring                                 4.17.4   True        False         False      11m     
    network                                    4.17.4   True        False         False      8d      
    node-tuning                                4.17.4   True        False         False      57m     
    openshift-apiserver                        4.17.4   True        False         False      10m     
    openshift-controller-manager               4.17.4   True        False         False      95m     
    openshift-samples                          4.17.4   True        False         False      59m     
    operator-lifecycle-manager                 4.17.4   True        False         False      8d      
    operator-lifecycle-manager-catalog         4.17.4   True        False         False      8d      
    operator-lifecycle-manager-packageserver   4.17.4   True        False         False      25m     
    service-ca                                 4.17.4   True        False         False      8d      
    storage                                    4.17.4   True        False         False      8d      
    



## Access the OpenShift Console

    [root@bastion ~]# oc get route -A
    NAMESPACE                  NAME                      HOST/PORT                                                           PATH        SERVICES            PORT    TERMINATION            WILDCARD
    openshift-authentication   oauth-openshift           oauth-openshift.apps.lab.example.com                                            oauth-openshift     6443    passthrough/Redirect   None
    openshift-console          console                   console-openshift-console.apps.lab.example.com                                  console             https   reencrypt/Redirect     None
    openshift-console          downloads                 downloads-openshift-console.apps.lab.example.com                                downloads           http    edge/Redirect          None
    openshift-ingress-canary   canary                    canary-openshift-ingress-canary.apps.lab.example.com                            ingress-canary      8080    edge/Redirect          None
    openshift-monitoring       alertmanager-main         alertmanager-main-openshift-monitoring.apps.lab.example.com         /api        alertmanager-main   web     reencrypt/Redirect     None
    openshift-monitoring       prometheus-k8s            prometheus-k8s-openshift-monitoring.apps.lab.example.com            /api        prometheus-k8s      web     reencrypt/Redirect     None
    openshift-monitoring       prometheus-k8s-federate   prometheus-k8s-federate-openshift-monitoring.apps.lab.example.com   /federate   prometheus-k8s      web     reencrypt/Redirect     None
    openshift-monitoring       thanos-querier            thanos-querier-openshift-monitoring.apps.lab.example.com            /api        thanos-querier      web     reencrypt/Redirect     None
    
    [root@bastion ~]# vim /etc/hosts 
    192.168.1.202 console-openshift-console.apps.lab.example.com oauth-openshift.apps.lab.example.com


## HTPasswd identity provider in OpenShift 4

Create an HTPasswd file by installing the htpasswd utility by installing the httpd-tools package:

    [root@registry]]# yum install httpd-tools 

Create or update an users.htpasswd file (note that the -c option will rewrite and truncate the file if already exists) with a user name and hashed password:

    [root@registry]]# htpasswd -c -B -b users.htpasswd rh-ee-sanuragi redhat@123

Create the HTPasswd Secret with the previously created users.htpasswd file

    [root@registry]]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd -n openshift-config 
    
Create a custom resource for an HTPasswd identity provider:

    [root@registry]]# oc edit oauth cluster 
    apiVersion: config.openshift.io/v1
    kind: OAuth
    metadata:
      annotations:
        include.release.openshift.io/ibm-cloud-managed: "true"
        include.release.openshift.io/self-managed-high-availability: "true"
        release.openshift.io/create-only: "true"
      creationTimestamp: "2024-11-06T16:24:28Z"
      generation: 1
      name: cluster
      ownerReferences:
      - apiVersion: config.openshift.io/v1
        kind: ClusterVersion
        name: version
        uid: b88e2fab-d5a6-4356-8aaf-38ecd23551c3
      resourceVersion: "1237"
      uid: 6d19f844-aa15-4ad3-98fe-b65c2dade6e7
    spec: 
      identityProviders:
      - name: Internal_SSO
        challenge: true
        login: true
        mappingMethod: claim
        type: HTPasswd
        htpasswd:
          fileData:
            name: htpass-secret
        
Getting oauth pod status. 

    [root@registry]]# oc get po -n openshift-authentication 
    NAME                               READY   STATUS    RESTARTS   AGE
    oauth-openshift-7965664bf5-j6dqg   1/1     Running   0          139m

Try to login with local user. if any issue occur, try this for best practice. 

    [root@registry]]# oc extract secret/router-ca --keys=tls.crt -n openshift-ingress-operator
    [root@registry]]# cp tls.crt /etc/pki/ca-trust/source/anchors/
    [root@registry]]# update-ca-trust 

    [root@registry]]# oc login -u rh-ee-sanuragi -p redhat@123
    Login successful.
    You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects'
    Using project "default".

    [root@bastion ~]# oc whoami
    rh-ee-sanuragi

    [root@bastion ~]# oc get identity
    NAME                          IDP NAME       IDP USER NAME    USER NAME        USER UID
    Internal_SSO:rh-ee-sanuragi   Internal_SSO   rh-ee-sanuragi   rh-ee-sanuragi   b7154e34-9ef9-42d4-8373-e9a2847ab1c4
    
## Removing the kubeadmin user

After you define an identity provider and create a new cluster-admin user, you can remove the kubeadmin to improve cluster security.

> **[!WARNING]**
> If you follow this procedure before another user is a cluster-admin, then OpenShift Container Platform must be reinstalled. It is not possible to undo this command.

### Prerequisites

* You must have configured at least one identity provider.
* You must have added the cluster-admin role to a user.
* You must be logged in as an administrator.

Retrieve the kubeadmin secret details as shown below
    
    [root@bastion ~]# oc get secrets kubeadmin -n kube-system
    NAME        TYPE     DATA   AGE
    kubeadmin   Opaque   1      97d


Remove the kubeadmin secret as shown below


    [root@bastion ~]# oc delete secrets kubeadmin -n kube-system
    secret "kubeadmin" deleted


Verify if the kubeadmin secret no more exists as shown below
    
    [root@bastion ~]# oc get secrets kubeadmin -n kube-system
    Error from server (NotFound): secrets "kubeadmin" not found
